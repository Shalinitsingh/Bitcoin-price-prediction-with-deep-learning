{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Importing the libraries and modules**\n",
    "\n",
    "from google.colab import files\n",
    "from bs4 import BeautifulSoup   #for web scraping \n",
    "import requests #for requesting html\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import re #regular expression for data extraction by pattern matching\n",
    "import pandas as pd # for dataframe\n",
    "from csv import reader #for list structure\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "Creating a list of URL\n",
    "\n",
    "#store feature names\n",
    "features =['transactions','size','sentbyaddress','difficulty','hashrate','mining_profitability','sentinusd','transactionfees','median_transaction_fee','confirmationtime','transactionvalue','mediantransactionvalue','activeaddresses','top100cap','fee_to_reward','price']\n",
    "\n",
    "url_list=[]\n",
    "\n",
    "\n",
    "for i in range(len(features)): #this loop generates urls\n",
    "      url='https://bitinfocharts.com/comparison/'+features[i]+'-'+'btc'+'.html'\n",
    "      \n",
    "      print(url)\n",
    "      url_list.append(url)\n",
    "      \n",
    "\n",
    "print(len(url_list))\n",
    "\n",
    "Scrape data from urls\n",
    "\n",
    "date = []\n",
    "transactions = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[0])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    date.append(x)\n",
    "    transactions.append(y)\n",
    "\n",
    "size = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[1])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    size.append(y)\n",
    "\n",
    "sentbyaddress = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[2])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    sentbyaddress.append(y)\n",
    "\n",
    "difficulty = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[3])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    difficulty.append(y)\n",
    "\n",
    "hashrate = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[4])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    hashrate.append(y)\n",
    "\n",
    "mining_profitability = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[5])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    mining_profitability.append(y)\n",
    "\n",
    "sentinusd = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[6])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    sentinusd.append(y)\n",
    "\n",
    "transactionfees = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[7])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    transactionfees.append(y)\n",
    "\n",
    "median_transaction_fee = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[8])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    median_transaction_fee.append(y)\n",
    "\n",
    "confirmationtime = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[9])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    confirmationtime.append(y)\n",
    "\n",
    "transactionvalue = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[10])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    transactionvalue.append(y)\n",
    "\n",
    "mediantransactionvalue = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[11])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    mediantransactionvalue.append(y)\n",
    "\n",
    "activeaddresses = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[12])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    activeaddresses.append(y)\n",
    "\n",
    "top100cap = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[13])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    top100cap.append(y)\n",
    "\n",
    "fee_to_reward = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[14])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    fee_to_reward.append(y)\n",
    "\n",
    "price = []\n",
    "session = requests.Session()\n",
    "page = session.get(url_list[15])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "values = str(soup.find_all('script')[4])\n",
    "values = values.split('d = new Dygraph(document.getElementById(\"container\"),')[1].split(', {labels: ')[0]\n",
    "\n",
    "for j in range(values.count('new Date')):\n",
    "    x = values.split('new Date(\"')[j+1].split('\"')[0]\n",
    "    y = values.split('\"),')[j+1].split(']')[0]\n",
    "    price.append(y)\n",
    "\n",
    "# printing for checking\n",
    "\n",
    "print(len(date))\n",
    "print(len(transactions))\n",
    "print(len(size))\n",
    "print(len(price))\n",
    "\n",
    "\n",
    "\n",
    "**Creating a Dataframe**\n",
    "\n",
    "printing all the value from list into a dataframe\n",
    "\n",
    "a = {'Date': date,'Transactions':transactions,'Size':size,'Sent by address':sentbyaddress,'Difficulty':difficulty,'Hashrate':difficulty,'Mining profitability':mining_profitability,'Sent inusd':sentinusd,'Transaction fees':transactionfees,'Median transaction fee':median_transaction_fee,'Confirmation time':confirmationtime,'Transaction value':transactionvalue,'Median transaction value':mediantransactionvalue,'Active addresses':activeaddresses,'Top 100 cap':top100cap,'Fee to reward':fee_to_reward,'Price':price}\n",
    "\n",
    "df = pd.DataFrame.from_dict(a, orient='index')\n",
    "df = df.transpose()\n",
    "df\n",
    "\n",
    "Download CSV file\n",
    "\n",
    "df.to_csv('Raw.csv', index=False,header=True, encoding='utf-8')\n",
    "files.download('Raw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32ac6d97661a52dd872279fb45f74324069e96b8e2b34c9b1544ec1ffb908f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
